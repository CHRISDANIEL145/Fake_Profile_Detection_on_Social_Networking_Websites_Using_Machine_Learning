{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio\n",
        "!pip install xgboost\n",
        "!pip install tensorflow"
      ],
      "metadata": {
        "id": "kHf5WgpyjtZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKIZdXkmjNPh"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import joblib\n",
        "import gradio as gr\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"insta_train.csv\")\n",
        "\n",
        "# Define features and target variable\n",
        "X = df.drop(columns=['fake'])  # Features\n",
        "y = df['fake']  # Target: 0 = Not Fake, 1 = Fake\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Save the scaler for later use\n",
        "joblib.dump(scaler, \"scaler.pkl\")\n",
        "\n",
        "# ============== BEST ML MODEL: XGBOOST ============== #\n",
        "xgb = XGBClassifier(n_estimators=300, learning_rate=0.05, max_depth=7, random_state=42)\n",
        "xgb.fit(X_train, y_train)\n",
        "xgb_pred = xgb.predict(X_test)\n",
        "xgb_acc = accuracy_score(y_test, xgb_pred)\n",
        "\n",
        "# ============== ANN MODEL ============== #\n",
        "model = Sequential([\n",
        "    Dense(128, input_dim=X_train.shape[1], activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.3),\n",
        "\n",
        "    Dense(256, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.3),\n",
        "\n",
        "    Dense(128, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.3),\n",
        "\n",
        "    Dense(2, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train_scaled, y_train, epochs=100, validation_data=(X_test_scaled, y_test), batch_size=32, verbose=1)\n",
        "\n",
        "# Evaluate ANN model\n",
        "ann_pred = np.argmax(model.predict(X_test_scaled), axis=1)\n",
        "ann_acc = accuracy_score(y_test, ann_pred)\n",
        "\n",
        "# Choose the best model\n",
        "best_model = \"XGBoost\" if xgb_acc > ann_acc else \"ANN\"\n",
        "best_acc = max(xgb_acc, ann_acc)\n",
        "print(f\"Best Model: {best_model} with Accuracy: {best_acc:.2f}\")\n",
        "\n",
        "# Save the ANN model\n",
        "model.save(\"fake_profile_model.h5\")\n",
        "\n",
        "# ============== GRADIO INTERFACE ============== #\n",
        "def predict_fake_profile(profile_pic, nums_length_username, fullname_words,\n",
        "                         nums_length_fullname, name_equals_username, description_length,\n",
        "                         external_url, private, num_posts, num_followers, num_follows):\n",
        "\n",
        "    # Convert input into a numpy array\n",
        "    user_input = np.array([[profile_pic, nums_length_username, fullname_words,\n",
        "                            nums_length_fullname, name_equals_username, description_length,\n",
        "                            external_url, private, num_posts, num_followers, num_follows]])\n",
        "\n",
        "    # Normalize input data\n",
        "    user_input_scaled = scaler.transform(user_input)\n",
        "\n",
        "    # Predict using the best model\n",
        "    if best_model == \"XGBoost\":\n",
        "        prediction = xgb.predict(user_input)\n",
        "    else:  # ANN\n",
        "        prediction = np.argmax(model.predict(user_input_scaled), axis=1)\n",
        "\n",
        "    return \"FAKE Profile (1)\" if prediction[0] == 1 else \"NOT FAKE Profile (0)\"\n",
        "\n",
        "# Create Gradio Interface\n",
        "iface = gr.Interface(\n",
        "    fn=predict_fake_profile,\n",
        "    inputs=[\n",
        "        gr.Radio([1, 0], label=\"Profile Pic (1 for Yes, 0 for No)\"),\n",
        "        gr.Number(label=\"Nums/Length Username\"),\n",
        "        gr.Number(label=\"Fullname Words\"),\n",
        "        gr.Number(label=\"Nums/Length Fullname\"),\n",
        "        gr.Radio([1, 0], label=\"Name == Username (1 for Yes, 0 for No)\"),\n",
        "        gr.Number(label=\"Description Length\"),\n",
        "        gr.Radio([1, 0], label=\"External URL (1 for Yes, 0 for No)\"),\n",
        "        gr.Radio([1, 0], label=\"Private (1 for Yes, 0 for No)\"),\n",
        "        gr.Number(label=\"# Posts\"),\n",
        "        gr.Number(label=\"# Followers\"),\n",
        "        gr.Number(label=\"# Follows\")\n",
        "    ],\n",
        "    outputs=\"text\",\n",
        "    title=\"Fake Profile Detection\",\n",
        "    description=f\"Best Model Selected: {best_model} with Accuracy: {best_acc:.2f}\\nEnter the profile details to check if it's FAKE (1) or NOT FAKE (0).\",\n",
        ")\n",
        "\n",
        "# Launch Gradio app\n",
        "iface.launch()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow gradio xgboost scikit-learn pandas joblib matplotlib seaborn\n"
      ],
      "metadata": {
        "id": "BpKpj3XjkEc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import joblib\n",
        "import gradio as gr\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"insta_train.csv\")\n",
        "\n",
        "# Define features and target variable\n",
        "X = df.drop(columns=['fake'])  # Features\n",
        "y = df['fake']  # Target: 0 = Not Fake, 1 = Fake\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Save the scaler for later use\n",
        "joblib.dump(scaler, \"scaler.pkl\")\n",
        "\n",
        "# ============== BEST ML MODEL: XGBOOST ============== #\n",
        "xgb = XGBClassifier(n_estimators=300, learning_rate=0.05, max_depth=7, random_state=42)\n",
        "xgb.fit(X_train, y_train)\n",
        "xgb_pred = xgb.predict(X_test)\n",
        "xgb_acc = accuracy_score(y_test, xgb_pred)\n",
        "\n",
        "# ============== ANN MODEL ============== #\n",
        "model = Sequential([\n",
        "    Dense(128, input_dim=X_train.shape[1], activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.3),\n",
        "\n",
        "    Dense(256, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.3),\n",
        "\n",
        "    Dense(128, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.3),\n",
        "\n",
        "    Dense(2, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train_scaled, y_train, epochs=100, validation_data=(X_test_scaled, y_test), batch_size=32, verbose=1)\n",
        "\n",
        "# Evaluate ANN model\n",
        "ann_pred = np.argmax(model.predict(X_test_scaled), axis=1)\n",
        "ann_acc = accuracy_score(y_test, ann_pred)\n",
        "\n",
        "# Choose the best model\n",
        "best_model = \"XGBoost\" if xgb_acc > ann_acc else \"ANN\"\n",
        "best_acc = max(xgb_acc, ann_acc)\n",
        "print(f\"Best Model: {best_model} with Accuracy: {best_acc:.2f}\")\n",
        "\n",
        "# Save the ANN model\n",
        "model.save(\"fake_profile_model.h5\")\n",
        "\n",
        "# ============== DATA VISUALIZATION ============== #\n",
        "# 1Ô∏è‚É£ Confusion Matrix\n",
        "def plot_confusion_matrix():\n",
        "    cm = confusion_matrix(y_test, xgb_pred if best_model == \"XGBoost\" else ann_pred)\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Not Fake\", \"Fake\"], yticklabels=[\"Not Fake\", \"Fake\"])\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"Actual\")\n",
        "    plt.title(f\"Confusion Matrix - {best_model}\")\n",
        "    plt.show()\n",
        "\n",
        "# 2Ô∏è‚É£ Feature Importance (XGBoost)\n",
        "if best_model == \"XGBoost\":\n",
        "    def plot_feature_importance():\n",
        "        importance = xgb.feature_importances_\n",
        "        features = X.columns\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.barplot(x=importance, y=features, palette=\"coolwarm\")\n",
        "        plt.title(\"Feature Importance (XGBoost)\")\n",
        "        plt.xlabel(\"Importance Score\")\n",
        "        plt.ylabel(\"Features\")\n",
        "        plt.show()\n",
        "\n",
        "# 3Ô∏è‚É£ Accuracy & Loss Curve (ANN)\n",
        "def plot_training_curves():\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    # Plot Accuracy\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history[\"accuracy\"], label=\"Train Accuracy\")\n",
        "    plt.plot(history.history[\"val_accuracy\"], label=\"Validation Accuracy\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.title(\"Model Accuracy Over Epochs\")\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot Loss\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history[\"loss\"], label=\"Train Loss\")\n",
        "    plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"Model Loss Over Epochs\")\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# ============== GRADIO INTERFACE ============== #\n",
        "def predict_fake_profile(profile_pic, nums_length_username, fullname_words,\n",
        "                         nums_length_fullname, name_equals_username, description_length,\n",
        "                         external_url, private, num_posts, num_followers, num_follows):\n",
        "\n",
        "    # Convert input into a numpy array\n",
        "    user_input = np.array([[profile_pic, nums_length_username, fullname_words,\n",
        "                            nums_length_fullname, name_equals_username, description_length,\n",
        "                            external_url, private, num_posts, num_followers, num_follows]])\n",
        "\n",
        "    # Normalize input data\n",
        "    user_input_scaled = scaler.transform(user_input)\n",
        "\n",
        "    # Predict using the best model\n",
        "    if best_model == \"XGBoost\":\n",
        "        prediction = xgb.predict(user_input)\n",
        "    else:  # ANN\n",
        "        prediction = np.argmax(model.predict(user_input_scaled), axis=1)\n",
        "\n",
        "    # Return output message\n",
        "    if prediction[0] == 1:\n",
        "        return f\"üö® **FAKE Profile!** üö®\\n‚ö†Ô∏è **Ban the ID as soon as possible!**\\n\\nüî¢ Accuracy Score: {best_acc:.2f}\"\n",
        "    else:\n",
        "        return f\"‚úÖ **NO Fake Profile** ‚úÖ\\nüëç **The account is safe!**\\n\\nüî¢ Accuracy Score: {best_acc:.2f}\"\n",
        "\n",
        "# Create Gradio Interface\n",
        "iface = gr.Interface(\n",
        "    fn=predict_fake_profile,\n",
        "    inputs=[\n",
        "        gr.Radio([1, 0], label=\"Profile Pic (1 for Yes, 0 for No)\"),\n",
        "        gr.Number(label=\"Nums/Length Username\"),\n",
        "        gr.Number(label=\"Fullname Words\"),\n",
        "        gr.Number(label=\"Nums/Length Fullname\"),\n",
        "        gr.Radio([1, 0], label=\"Name == Username (1 for Yes, 0 for No)\"),\n",
        "        gr.Number(label=\"Description Length\"),\n",
        "        gr.Radio([1, 0], label=\"External URL (1 for Yes, 0 for No)\"),\n",
        "        gr.Radio([1, 0], label=\"Private (1 for Yes, 0 for No)\"),\n",
        "        gr.Number(label=\"# Posts\"),\n",
        "        gr.Number(label=\"# Followers\"),\n",
        "        gr.Number(label=\"# Follows\")\n",
        "    ],\n",
        "    outputs=\"text\",\n",
        "    title=\"Fake Profile Detection\",\n",
        "    description=f\"üîç **Best Model Selected: {best_model}** (Accuracy: {best_acc:.2f})\\nüìå Enter the profile details to check if it's FAKE (üö®) or NOT FAKE (‚úÖ).\",\n",
        ")\n",
        "\n",
        "# Show visualizations\n",
        "plot_confusion_matrix()\n",
        "if best_model == \"XGBoost\":\n",
        "    plot_feature_importance()\n",
        "if best_model == \"ANN\":\n",
        "    plot_training_curves()\n",
        "\n",
        "# Launch Gradio app\n",
        "iface.launch()\n"
      ],
      "metadata": {
        "id": "qU2ZEgg9mjmv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i6z7_SJJmovz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}